{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow\n",
    "np.random.seed(0)\n",
    "tensorflow.random.set_seed(0)\n",
    "from scipy import stats\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "from itertools import combinations, product\n",
    "import glob\n",
    "import os.path\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential, Model\n",
    "from tensorflow.keras import layers\n",
    "from keras.layers import Dense, LSTM, Dropout,GRU,TimeDistributed, SimpleRNN,Bidirectional,RepeatVector,Flatten\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['L', 'M', 'H']\n",
    "\n",
    "# Task\n",
    "task_name = 'ComParE2020_USOMS-e'\n",
    "\n",
    "# Enter your team name HERE\n",
    "team_name = 'NYIT'\n",
    "\n",
    "# Enter your submission number HERE\n",
    "submission_index = 1\n",
    "# Option\n",
    "show_confusion = True  # Display confusion matrix on devel\n",
    "majority_vote_story_id = True # Perform a majority vote over all audio file prediction, based on SpeakerID + Story\n",
    "# Configuration\n",
    "\n",
    "complexities = [1e-5,1e-4,1e-3,1e-2,1e-1,1e0]\n",
    "\n",
    "# Mapping each available feature set to tuple (number of features, offset/index of first feature, separator, header option)\n",
    "feat_conf_c = {'ComParE':      (6373, 1, ';', 'infer')}\n",
    "feat_conf_d = {'DeepSpectrum_resnet50': (2048, 1, ',', 'infer')}\n",
    "\n",
    "feat_conf_b = {'BoAW-125':     ( 250, 1, ';',  None),\n",
    "               'BoAW-250':     ( 500, 1, ';',  None),\n",
    "               'BoAW-500':     (1000, 1, ';',  None),\n",
    "               'BoAW-1000':    (2000, 1, ';',  None),\n",
    "               'BoAW-2000':    (4000, 1, ';',  None)}\n",
    "\n",
    "feat_conf_a ={'auDeep-30':    (1024, 2, ',', 'infer'),\n",
    "              'auDeep-45':    (1024, 2, ',', 'infer'),\n",
    "              'auDeep-60':    (1024, 2, ',', 'infer'),\n",
    "              'auDeep-75':    (1024, 2, ',', 'infer'),\n",
    "              'auDeep-fused': (4096, 2, ',', 'infer')}\n",
    "\n",
    "fisher_path = './fisher_vector/'\n",
    "fisher_option= [8,16,32,64]\n",
    "\n",
    "feat_conf_f = {f'fisher_vector_{fisher_option[0]}': (),\n",
    "               f'fisher_vector_{fisher_option[1]}': (),\n",
    "               f'fisher_vector_{fisher_option[2]}': (),\n",
    "               f'fisher_vector_{fisher_option[3]}': ()}\n",
    "\n",
    "# adding early fusion features\n",
    "feat_conf_vv = {'auDeep-45-BoAW-500-ComParE-DeepSpectrum_resnet50': (),\n",
    "               'DeepSpectrum_resnet50-auDeep-75-BoAW-250': (),\n",
    "               'auDeep-60-BoAW-250-ComParE-DeepSpectrum_resnet50': ()}\n",
    "feat_conf_aa = {'DeepSpectrum_resnet50-auDeep-75-BoAW-125': (),\n",
    "               'DeepSpectrum_resnet50-auDeep-75-BoAW-250': (),\n",
    "               'DeepSpectrum_resnet50-auDeep-60-BoAW-250': ()}\n",
    "\n",
    "feat_conf_vcat = {**feat_conf_a,**feat_conf_b,**feat_conf_c,**feat_conf_d,**feat_conf_f,**feat_conf_vv}\n",
    "feat_conf_acat = {**feat_conf_a,**feat_conf_b,**feat_conf_c,**feat_conf_d,**feat_conf_f,**feat_conf_aa}\n",
    "\n",
    "# Path of the features and labels\n",
    "features_path = '../features/'\n",
    "label_file = '../lab/labels.csv'\n",
    "df_labels = pd.read_csv(label_file)\n",
    "\n",
    "\n",
    "# Labels\n",
    "label_options = ['V_cat','A_cat']\n",
    "# ,'V_cat'\n",
    "UAR_A_cat = []\n",
    "UAR_V_cat = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_conf = [feat_conf_vcat,feat_conf_acat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading csv file path\n",
    "\n",
    "train_file = []\n",
    "devel_file = []\n",
    "for s in range(len(label_options)):\n",
    "    current_label = label_options[s]\n",
    "    csv_train = []\n",
    "    csv_devel = [] \n",
    "    for i in list(dict.keys(feat_conf[s])):\n",
    "            current_feature = i\n",
    "            print(i)\n",
    "            csv_train+= glob.glob(os.path.join('.\\\\Decision_Fun_CSV\\\\Train\\\\', f\"{current_feature}_{current_label}_*\"))\n",
    "            csv_devel+= glob.glob(os.path.join('.\\\\Decision_Fun_CSV\\\\Devel\\\\', f\"{current_feature}_{current_label}_*\"))\n",
    "    train_file.append(csv_train)\n",
    "    devel_file.append(csv_devel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Features\n",
    "\n",
    "X_rnn_train =[]\n",
    "X_rnn_devel =[]\n",
    "for t_f, d_f in zip(train_file,devel_file):\n",
    "    X_f_train = []\n",
    "    X_f_devel = []\n",
    "    for s_f, e_f in zip(t_f,d_f):\n",
    "        temp_train = pd.read_csv(s_f,index_col=0).values\n",
    "        temp_devel = pd.read_csv(e_f,index_col=0).values\n",
    "        X_f_train.append(temp_train)\n",
    "        X_f_devel.append(temp_devel)\n",
    "    X_rnn_train.append(X_f_train)\n",
    "    X_rnn_devel.append(X_f_devel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN & LSTM Algorithm Implementation\n",
    "\n",
    "feature_best_UAR_cat = []\n",
    "prob_funsion_cat = []\n",
    "UAR_p_a =[]\n",
    "UAR_p_v =[]\n",
    "cfs_a = []\n",
    "cfs_v = []\n",
    "pred_a_train=[]\n",
    "pred_v_train=[]\n",
    "pred_a_devel=[]\n",
    "pred_v_devel=[]\n",
    "\n",
    "for count1 in range(len(X_rnn_train)) :\n",
    "    current_label = label_options[count1]\n",
    "    print('\\nRunning ' + task_name+' '+current_label+ ' LSTM' + ' baseline ... (this might take a while) \\n')\n",
    "\n",
    "    feature_best_UAR = []\n",
    "    prob_story_fusion = []\n",
    "    UAR_p = []\n",
    "    cfs_matrix = []\n",
    "    predic_train = []\n",
    "    predic_devel = []\n",
    "    for i in range(len(X_rnn_train[count1])):\n",
    "        current_feature = list(dict.keys(feat_conf[count1]))[i]\n",
    "        \n",
    "        y_train = df_labels[current_label][df_labels['filename_audio'].str.startswith('train')].values\n",
    "        y_devel = df_labels[current_label][df_labels['filename_audio'].str.startswith('devel')].values\n",
    "        train_audio = df_labels['filename_audio'][df_labels['filename_audio'].str.startswith('train')].values\n",
    "        train_text =  df_labels['filename_text'][df_labels['filename_audio'].str.startswith('train')].values\n",
    "\n",
    "        # Upsampling / Balancing\n",
    "        print('Upsampling ... ')\n",
    "        num_samples_train = []\n",
    "        for label in classes:\n",
    "            num_samples_train.append(len(y_train[y_train == label]))\n",
    "        for label, ns_tr in zip(classes, num_samples_train):\n",
    "            factor_tr = np.max(num_samples_train) // ns_tr\n",
    "            train_audio = np.concatenate((train_audio, np.tile(train_audio[y_train == label], (factor_tr - 1))))\n",
    "            train_text = np.concatenate((train_text, np.tile(train_text[y_train == label], (factor_tr - 1))))\n",
    "            y_train = np.concatenate((y_train, np.tile(y_train[y_train == label], (factor_tr - 1))))\n",
    "       \n",
    "        scaler = MinMaxScaler()\n",
    "        X_train = scaler.fit_transform(X_rnn_train[count1][i])\n",
    "        X_devel = scaler.transform(X_rnn_devel[count1][i])\n",
    "    \n",
    "        X_train = pd.DataFrame(X_train)\n",
    "        X_devel = pd.DataFrame(X_devel)\n",
    "        \n",
    "        df_with_stories_train= pd.DataFrame(\n",
    "        data={'filename_audio': train_audio,\n",
    "          'filename_text': train_text,  # filename_text == ID_Story\n",
    "          'L': X_train.iloc[:, 1],\n",
    "          'M': X_train.iloc[:, 2],\n",
    "          'H': X_train.iloc[:, 0],\n",
    "          'true': y_train.flatten()},columns=['filename_audio', 'filename_text', 'L', 'M','H','true'])\n",
    "\n",
    "        df_with_stories_devel= pd.DataFrame(\n",
    "        data={'filename_audio': df_labels['filename_audio'][df_labels['filename_audio'].str.startswith('devel')].values,\n",
    "          'filename_text': df_labels['filename_text'][df_labels['filename_audio'].str.startswith('devel')].values,  # filename_text == ID_Story\n",
    "          'L': X_devel.iloc[:, 1],\n",
    "          'M': X_devel.iloc[:, 2],\n",
    "          'H': X_devel.iloc[:, 0],\n",
    "          'true': y_devel.flatten()},columns=['filename_audio', 'filename_text', 'L', 'M','H','true'])\n",
    "       \n",
    "        #create feature with statistics of the probabilities of each label class for each story \n",
    "        stories_train = df_with_stories_train['filename_text'].unique()\n",
    "        stories_devel= df_with_stories_devel['filename_text'].unique()\n",
    "        max_length_train = sorted(df_with_stories_train.groupby(['filename_text'])['filename_text'].value_counts(),reverse=True)[0]\n",
    "        max_length_devel = sorted(df_with_stories_devel.groupby(['filename_text'])['filename_text'].value_counts(),reverse=True)[0]\n",
    "        feature_stories_train = []\n",
    "        feature_stories_devel = []\n",
    "        \n",
    "        print('Working on Train_story_prob')\n",
    "        for story in stories_train:\n",
    "            story_senten_prob_train = pd.DataFrame()\n",
    "            sentences = df_with_stories_train.groupby(['filename_text'])['L','M','H'].get_group(story).to_numpy()\n",
    "            s_length = sentences.shape[0]\n",
    "            dim_p = max_length_train-s_length\n",
    "            sentences = np.vstack((sentences,pd.np.zeros([dim_p,3])))\n",
    "            feature_stories_train.append(sentences)\n",
    "\n",
    "        print('Working on Devel_story_prob')    \n",
    "        for story in stories_devel:\n",
    "            story_senten_prob_devel = pd.DataFrame()\n",
    "            sentences = df_with_stories_devel.groupby(['filename_text'])['L','M','H'].get_group(story).to_numpy()\n",
    "            s_length = sentences.shape[0]\n",
    "            dim_p = max_length_train-s_length\n",
    "            sentences = np.vstack((sentences,pd.np.zeros([dim_p,3])))\n",
    "            feature_stories_devel.append(sentences)            \n",
    "        \n",
    "        story_label_train = df_with_stories_train.groupby(['filename_text'])['true'].agg(\n",
    "            lambda x: x.value_counts().sort_index().sort_values(ascending=False, kind='mergesort').index[0]).values\n",
    "\n",
    "        story_label_devel = df_with_stories_devel.groupby(['filename_text'])['true'].agg(\n",
    "            lambda x: x.value_counts().sort_index().sort_values(ascending=False, kind='mergesort').index[0]).values\n",
    "        \n",
    "\n",
    "        lb = LabelBinarizer()\n",
    "        binLabel_train = lb.fit_transform(story_label_train)\n",
    "        binLabel_devel = lb.transform(story_label_devel)\n",
    "        \n",
    "\n",
    "        feature_stories_train = np.asarray(feature_stories_train)\n",
    "        feature_stories_devel = np.asarray(feature_stories_devel)\n",
    "        \n",
    "        para = []\n",
    "        uar_SVM_story = []\n",
    "        prob_SVM_story = []\n",
    "        \n",
    "        \n",
    "        print(f'current feature sets: '+current_feature)\n",
    "        model = Sequential()\n",
    "        model.add(LSTM(64,kernel_regularizer=l2(0.0003),recurrent_regularizer=l2(0.0003),input_shape=(feature_stories_train.shape[1],feature_stories_train.shape[2])))\n",
    "#         model.add(LSTM(8,kernel_regularizer=l2(0.0003),recurrent_regularizer=l2(0.0003))),return_sequences=True\n",
    "        \n",
    "        model.add(Dropout(0.3))\n",
    "        model.add(Dense(3, activation='softmax'))\n",
    "        optimizer = optimizers.Adam(learning_rate=0.001)\n",
    "        model.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['accuracy'])\n",
    "        history = model.fit(feature_stories_train, binLabel_train, epochs=200, batch_size=32, validation_data=(feature_stories_devel, binLabel_devel), verbose=1, shuffle=False)\n",
    "\n",
    "        plt.plot(history.history['loss'], label='train')\n",
    "        plt.plot(history.history['val_loss'], label='test')\n",
    "        plt.legend()\n",
    "        save_fig(f'feature sets_{current_feature}_{current_label}_training_LSTM')\n",
    "        plt.show()\n",
    "\n",
    "        y_pred_p_train = model.predict(feature_stories_train)\n",
    "        y_pred_p_devel = model.predict(feature_stories_devel)\n",
    "                \n",
    "        y_pred_f_train = []\n",
    "        classes_pred=['H','L','M']\n",
    "        for r in range(len(y_pred_p_train)):\n",
    "            y_pred_f_train.extend(classes_pred[np.argmax(y_pred_p_train[r])])\n",
    "\n",
    "        \n",
    "        y_pred_f_devel = []\n",
    "        for s in range(len(y_pred_p_devel)):\n",
    "            y_pred_f_devel.extend(classes_pred[np.argmax(y_pred_p_devel[s])])\n",
    "            \n",
    "        y_test_p_r = lb.inverse_transform(binLabel_devel)\n",
    "        \n",
    "        score_p = recall_score(y_test_p_r, y_pred_f_devel, labels=classes, average='macro')\n",
    "        UAR_p.append(score_p)\n",
    "        print(score_p*100)\n",
    "        print(classes)\n",
    "        print(confusion_matrix(y_test_p_r, y_pred_f_devel, labels=classes))\n",
    "        cfs_matrix.append(confusion_matrix(y_test_p_r, y_pred_f_devel, labels=classes))\n",
    "        predic_train.append(y_pred_p_train)\n",
    "        predic_devel.append(y_pred_p_devel)\n",
    "\n",
    "    \n",
    "    if(current_label=='A_cat'):\n",
    "        UAR_p_a.append(UAR_p)\n",
    "        cfs_a.append(cfs_matrix)\n",
    "        pred_a_train.append(predic_train)\n",
    "        pred_a_devel.append(predic_devel)\n",
    "    else:\n",
    "        UAR_p_v.append(UAR_p)\n",
    "        cfs_v.append(cfs_matrix)\n",
    "        pred_v_train.append(predic_train)\n",
    "        pred_v_devel.append(predic_devel)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present all A_cat Result\n",
    "\n",
    "sorted(zip(UAR_p_a[0],feat_conf[1]),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present all V_cat Result\n",
    "\n",
    "sorted(zip(UAR_p_v[0],feat_conf[0]),reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present Top 3 A_cat Result & Their confusion Matrix\n",
    "\n",
    "print('A_cat Top 3:')\n",
    "for x in range(3):\n",
    "    print(f'Feature: {sorted(zip(UAR_p_a[0],feat_conf[1]),reverse=True)[x][1]}, Score: {sorted(zip(UAR_p_a[0],feat_conf[1]),reverse=True)[x][0]}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(classes)\n",
    "    print(cfs_a[0][list(dict.keys(feat_conf[1])).index(sorted(zip(UAR_p_a[0],feat_conf[1]),reverse=True)[x][1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present Top 3 V_cat Result & Their confusion Matrix\n",
    "\n",
    "print('V_cat Top 3:')\n",
    "for x in range(3):\n",
    "    print(f'Feature: {sorted(zip(UAR_p_v[0],feat_conf[0]),reverse=True)[x][1]}, Score: {sorted(zip(UAR_p_v[0],feat_conf[0]),reverse=True)[x][0]}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(classes)\n",
    "    print(cfs_a[0][list(dict.keys(feat_conf[0])).index(sorted(zip(UAR_p_v[0],feat_conf[0]),reverse=True)[x][1])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Fusion Features\n",
    "\n",
    "fu_a_train = np.array(pred_a_train[0])\n",
    "fu_a_devel = np.array(pred_a_devel[0])\n",
    "fu_v_train = np.array(pred_a_train[0])\n",
    "fu_v_devel = np.array(pred_a_devel[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option for Feature Selection\n",
    "\n",
    "useful_a=[]\n",
    "useful_v=[]\n",
    "thresh = 0.38\n",
    "top = 3\n",
    "# best = True\n",
    "best = False # Use All feature or Top 3\n",
    "if best==False:\n",
    "    for i in range(len(sorted(zip(UAR_p_a[0],feat_conf[1]),reverse=True))):\n",
    "        if sorted(zip(UAR_p_a[0],feat_conf[0]),reverse=True)[i][0]>thresh:\n",
    "            useful_a.append(sorted(zip(UAR_p_a[0],feat_conf[1]),reverse=True)[i][1])\n",
    "    for i in range(len(sorted(zip(UAR_p_v[0],feat_conf[1]),reverse=True))):\n",
    "        if sorted(zip(UAR_p_v[0],feat_conf[0]),reverse=True)[i][0]>thresh:\n",
    "            useful_v.append(sorted(zip(UAR_p_v[0],feat_conf[1]),reverse=True)[i][1])\n",
    "else:\n",
    "    for i in range(top):\n",
    "        useful_a.append(sorted(zip(UAR_p_a[0],feat_conf[1]),reverse=True)[i][1])\n",
    "        useful_v.append(sorted(zip(UAR_p_v[0],feat_conf[1]),reverse=True)[i][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fusion Settings\n",
    "\n",
    "feat_conf_c_fu = {'ComParE':  (fu_a_train[10],fu_a_devel[10],fu_v_train[10],fu_v_devel[10])}\n",
    "feat_conf_d_fu = {'DeepSpectrum_resnet50': (fu_a_train[11],fu_a_devel[12],fu_v_train[11],fu_v_devel[11])}\n",
    "\n",
    "feat_conf_b_fu = {'BoAW-125':   (fu_a_train[5],fu_a_devel[5],fu_v_train[5],fu_v_devel[5]),\n",
    "               'BoAW-250':     (fu_a_train[6],fu_a_devel[6],fu_v_train[6],fu_v_devel[6]),\n",
    "               'BoAW-500':     (fu_a_train[7],fu_a_devel[7],fu_v_train[7],fu_v_devel[7]),\n",
    "               'BoAW-1000':    (fu_a_train[8],fu_a_devel[8],fu_v_train[8],fu_v_devel[8]),\n",
    "               'BoAW-2000':    (fu_a_train[9],fu_a_devel[9],fu_v_train[9],fu_v_devel[9])}\n",
    "\n",
    "feat_conf_a_fu ={'auDeep-30': (fu_a_train[0],fu_a_devel[0],fu_v_train[0],fu_v_devel[0]),\n",
    "              'auDeep-45':    (fu_a_train[1],fu_a_devel[1],fu_v_train[1],fu_v_devel[1]),\n",
    "              'auDeep-60':    (fu_a_train[2],fu_a_devel[2],fu_v_train[2],fu_v_devel[2]),\n",
    "              'auDeep-75':    (fu_a_train[3],fu_a_devel[3],fu_v_train[3],fu_v_devel[3]),\n",
    "              'auDeep-fused': (fu_a_train[4],fu_a_devel[4],fu_v_train[4],fu_v_devel[4])}\n",
    "\n",
    "feat_conf_f_fu = {f'fisher_vector_{fisher_option[0]}': (fu_a_train[12],fu_a_devel[12],fu_v_train[12],fu_v_devel[12]),\n",
    "               f'fisher_vector_{fisher_option[1]}': (fu_a_train[13],fu_a_devel[13],fu_v_train[13],fu_v_devel[13]),\n",
    "               f'fisher_vector_{fisher_option[2]}': (fu_a_train[14],fu_a_devel[14],fu_v_train[14],fu_v_devel[14]),\n",
    "               f'fisher_vector_{fisher_option[3]}': (fu_a_train[15],fu_a_devel[15],fu_v_train[15],fu_v_devel[15])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing Label for Each datasets on each Category\n",
    "\n",
    "y_train_a = df_labels['A_cat'][df_labels['filename_audio'].str.startswith('train')].values\n",
    "df_with_stories_train_a= pd.DataFrame(\n",
    "        data={'filename_audio': df_labels['filename_audio'][df_labels['filename_audio'].str.startswith('train')].values,\n",
    "          'filename_text': df_labels['filename_text'][df_labels['filename_audio'].str.startswith('train')].values,  # filename_text == ID_Story\n",
    "          'true': y_train_a.flatten()},columns=['filename_audio', 'filename_text', 'L', 'M','H','true'])\n",
    "       \n",
    "story_label_train_a = df_with_stories_train_a.groupby(['filename_text'])['true'].agg(\n",
    "            lambda x: x.value_counts().sort_index().sort_values(ascending=False, kind='mergesort').index[0]).values\n",
    "y_train_v = df_labels['V_cat'][df_labels['filename_audio'].str.startswith('train')].values\n",
    "df_with_stories_train_v= pd.DataFrame(\n",
    "        data={'filename_audio': df_labels['filename_audio'][df_labels['filename_audio'].str.startswith('train')].values,\n",
    "          'filename_text': df_labels['filename_text'][df_labels['filename_audio'].str.startswith('train')].values,  # filename_text == ID_Story\n",
    "          'true': y_train_v.flatten()},columns=['filename_audio', 'filename_text', 'L', 'M','H','true'])\n",
    "       \n",
    "story_label_train_v = df_with_stories_train_v.groupby(['filename_text'])['true'].agg(\n",
    "            lambda x: x.value_counts().sort_index().sort_values(ascending=False, kind='mergesort').index[0]).values\n",
    "\n",
    "y_devel_a = df_labels['A_cat'][df_labels['filename_audio'].str.startswith('devel')].values\n",
    "df_with_stories_devel_a= pd.DataFrame(\n",
    "        data={'filename_audio': df_labels['filename_audio'][df_labels['filename_audio'].str.startswith('devel')].values,\n",
    "          'filename_text': df_labels['filename_text'][df_labels['filename_audio'].str.startswith('devel')].values,  # filename_text == ID_Story\n",
    "          'true': y_devel_a.flatten()},columns=['filename_audio', 'filename_text', 'L', 'M','H','true'])\n",
    "       \n",
    "story_label_devel_a = df_with_stories_devel_a.groupby(['filename_text'])['true'].agg(\n",
    "            lambda x: x.value_counts().sort_index().sort_values(ascending=False, kind='mergesort').index[0]).values\n",
    "\n",
    "y_devel_v = df_labels['V_cat'][df_labels['filename_audio'].str.startswith('devel')].values\n",
    "df_with_stories_devel_v= pd.DataFrame(\n",
    "        data={'filename_audio': df_labels['filename_audio'][df_labels['filename_audio'].str.startswith('devel')].values,\n",
    "          'filename_text': df_labels['filename_text'][df_labels['filename_audio'].str.startswith('devel')].values,  # filename_text == ID_Story\n",
    "          'true': y_devel_v.flatten()},columns=['filename_audio', 'filename_text', 'L', 'M','H','true'])\n",
    "       \n",
    "story_label_devel_v = df_with_stories_devel_v.groupby(['filename_text'])['true'].agg(\n",
    "            lambda x: x.value_counts().sort_index().sort_values(ascending=False, kind='mergesort').index[0]).values\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge Features and Label\n",
    "\n",
    "X_train_fu_DNN=[X_train_fu_DNN_a,X_train_fu_DNN_v]\n",
    "X_devel_fu_DNN=[X_devel_fu_DNN_a,X_devel_fu_DNN_v]\n",
    "story_label_train=[story_label_train_a,story_label_train_v]\n",
    "story_label_devel=[story_label_devel_a,story_label_devel_v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DNN Model Implementation\n",
    "\n",
    "cat = ['A_cat','V_cat']\n",
    "UAR_p_DNN = []\n",
    "cfs_matrix_DNN = []\n",
    "predic_DNN = []\n",
    "for l in range(len(cat)):\n",
    "    X_train_DNN = X_train_fu_DNN[l]\n",
    "    X_devel_DNN = X_devel_fu_DNN[l]\n",
    "    y_train_label_DNN = story_label_train[l]\n",
    "    y_devel_label_DNN = story_label_devel[l]\n",
    "\n",
    "    y_train_label_DNN_bin = lb.fit_transform(y_train_label_DNN)\n",
    "    y_devel_label_DNN_bin = lb.transform(y_devel_label_DNN)\n",
    "\n",
    "    model1 = Sequential()\n",
    "    model1.add(Dense(64,input_shape=(X_train_DNN.shape[1],)))\n",
    "\n",
    "    model1.add(Dense(3, activation='softmax'))\n",
    "    optimizer = optimizers.Adam(learning_rate=0.0001)\n",
    "    model1.compile(loss='categorical_crossentropy',optimizer=optimizer,metrics=['categorical_accuracy'])\n",
    "    history = model1.fit(X_train_DNN, y_train_label_DNN_bin, epochs=200, batch_size=25, validation_data=(X_devel_DNN, y_devel_label_DNN_bin), verbose=2,shuffle=False)\n",
    "\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='test')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    y_pred_p_DNN = model1.predict(X_devel_DNN)\n",
    "\n",
    "\n",
    "    y_pred_f_DNN = []\n",
    "    classes_pred=['H','L','M']\n",
    "    for s in range(len(y_pred_p_DNN)):\n",
    "        y_pred_f_DNN.extend(classes_pred[np.argmax(y_pred_p_DNN[s])])\n",
    "    y_devel_p_r = lb.inverse_transform(y_devel_label_DNN_bin)\n",
    "\n",
    "    score_p_DNN = recall_score(y_devel_p_r, y_pred_f_DNN, labels=classes, average='macro')\n",
    "    UAR_p_DNN.append(score_p_DNN)\n",
    "    print(cat[l])\n",
    "    print(score_p_DNN*100)\n",
    "    print(classes)\n",
    "    print(confusion_matrix(y_devel_p_r, y_pred_f_DNN, labels=classes))\n",
    "    cfs_matrix_DNN.append(confusion_matrix(y_devel_p_r, y_pred_f_DNN, labels=classes))\n",
    "    predic_DNN.append(y_pred_f_DNN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present DNN result\n",
    "\n",
    "UAR_p_DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Present Confusion Matrix\n",
    "\n",
    "cfs_matrix_DNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python37364bitbaseconda31ecf04f0a8c44b29107d72eaf206cf0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}